# LuminaGuard Agent

The Python reasoning layer for LuminaGuard. Handles agent decision-making, LLM integration, messenger connectors, and the 24/7 bot runtime.

---

## ğŸš€ Quickstart â€” Create a 24/7 Bot

The fastest path to a running bot. No Firecracker, no KVM, no Rust build required.

### 1. Install dependencies

```bash
cd agent
python3 -m venv .venv && source .venv/bin/activate
pip install -e ".[dev]"
```

### 2. Run the bot

```bash
# Check your setup status
python create_bot.py --status

# Send a one-shot message
python create_bot.py --message "Hello"
# â†’ Please setup environment variables for your LLM

# Start an interactive REPL
python create_bot.py
```

### 3. Enable AI responses

Set at least one LLM provider environment variable:

```bash
export OPENAI_API_KEY=sk-â€¦          # OpenAI / GPT
export ANTHROPIC_API_KEY=sk-ant-â€¦   # Anthropic / Claude
export OLLAMA_HOST=http://localhost:11434  # Local Ollama (free)
```

Then run again â€” the bot auto-detects your provider.

### 4. Use from Python

```python
from bot_factory import create_bot

# Zero-config
bot = create_bot()
print(bot.chat("Hello"))

# Custom bot
bot = create_bot(
    bot_name="MyBot",
    username="alice",
    use_case="monitoring",
)
bot.run_repl()          # interactive REPL
bot.status()            # diagnostics dict
```

---

## ğŸ“¦ Key Modules

| File | Purpose |
|------|---------|
| [`bot_factory.py`](bot_factory.py) | **Start here.** `BotFactory`, `ReadyBot`, `BotConfig`, `create_bot()` |
| [`create_bot.py`](create_bot.py) | CLI script â€” `python create_bot.py --help` |
| [`llm_client.py`](llm_client.py) | LLM client factory, `get_bot_response()`, `is_llm_configured()` |
| [`loop.py`](loop.py) | Core agent reasoning loop (`run_loop`, `think`, `AgentState`) |
| [`daemon_config.py`](daemon_config.py) | Daemon configuration (`DaemonConfig`, `ConfigManager`) |
| [`daemon/persona.py`](daemon/persona.py) | Persona & onboarding (`PersonaManager`, `OnboardingFlow`) |
| [`messenger/`](messenger/) | Messenger connectors (Discord, Telegram, â€¦) |
| [`mcp_client.py`](mcp_client.py) | MCP client for Rust orchestrator communication |

---

## ğŸ¤– BotFactory API

### `create_bot()` â€” convenience function

```python
from bot_factory import create_bot

bot = create_bot(
    bot_name="LuminaBot",       # display name
    username="user",            # your username
    use_case="general",         # brief description
    config_dir=None,            # defaults to ~/.luminaguard/bot/
)
```

### `BotFactory.create(config)` â€” full control

```python
from bot_factory import BotFactory, BotConfig
from llm_client import LLMProvider

cfg = BotConfig(
    bot_name="MyBot",
    username="alice",
    llm_provider=LLMProvider.OPENAI,   # explicit override
    llm_api_key="sk-â€¦",
    llm_model="gpt-4o",
    extra_handlers=[my_async_handler],  # custom handlers (run first)
)
bot = BotFactory.create(cfg)
```

### `ReadyBot` methods

```python
bot.chat("Hello")           # sync â†’ str
await bot.achat("Hello")    # async â†’ str
bot.run_repl()              # interactive terminal REPL
bot.status()                # â†’ {"bot_name", "username", "llm_configured", "config_dir"}
```

### What `BotFactory.create()` does automatically

1. **Daemon config** â€” loads `DaemonConfig` with sensible defaults
2. **Persona** â€” loads from `~/.luminaguard/bot/` or creates a new one
3. **Onboarding profile** â€” loads or creates for the given username
4. **LLM client** â€” auto-detects from env vars (`OPENAI_API_KEY` â†’ OpenAI, `ANTHROPIC_API_KEY` â†’ Anthropic, `OLLAMA_HOST` â†’ Ollama), falls back to mock
5. **Message router** â€” wires `MessageRouter` with an LLM-backed default handler

---

## ğŸ”§ CLI Reference (`create_bot.py`)

```
python create_bot.py [OPTIONS]

Options:
  --name, -n NAME        Bot display name (default: LuminaBot)
  --username, -u USER    Your username (default: user)
  --use-case TEXT        Bot's purpose description
  --config-dir DIR       Persist persona/profile here (default: ~/.luminaguard/bot)
  --message, -m TEXT     One-shot: send TEXT and print reply, then exit
  --status, -s           Print LLM setup status and exit
  --verbose, -v          Enable verbose logging
```

**Examples:**

```bash
# Check what LLM is configured
python create_bot.py --status

# One-shot message (great for scripting / CI)
python create_bot.py --message "Hello"

# Named bot for a specific user
python create_bot.py --name "OpsBot" --username "ops-team" --use-case "infrastructure monitoring"

# Custom config directory
python create_bot.py --config-dir /etc/mybot --message "Hello"
```

---

## ğŸŒ Messenger Connectors

Connect the bot to Discord, Telegram, or other platforms via `messenger/server.py`:

```bash
# Discord
DISCORD_TOKEN=xxx python -m messenger.server --discord

# Telegram
TELEGRAM_TOKEN=yyy python -m messenger.server --telegram

# Both
DISCORD_TOKEN=xxx TELEGRAM_TOKEN=yyy python -m messenger.server
```

Or from a config file:

```bash
python -m messenger.server --config messenger.example.json
```

See [`messenger.example.json`](messenger.example.json) for the config format.

---

## ğŸ§ª Running Tests

```bash
# All agent tests
python -m pytest tests/ -v

# Just the bot factory / setup tests
python -m pytest tests/test_bot_factory.py tests/test_247_bot_setup.py -v

# With coverage
python -m pytest tests/ --cov=. --cov-report=term-missing
```

---

## ğŸ“ Directory Structure

```
agent/
â”œâ”€â”€ bot_factory.py          â† Start here for 24/7 bot creation
â”œâ”€â”€ create_bot.py           â† CLI entry point
â”œâ”€â”€ llm_client.py           â† LLM client factory + get_bot_response()
â”œâ”€â”€ loop.py                 â† Agent reasoning loop
â”œâ”€â”€ daemon_config.py        â† Daemon configuration
â”œâ”€â”€ mcp_client.py           â† MCP client
â”œâ”€â”€ daemon/
â”‚   â”œâ”€â”€ persona.py          â† Persona & onboarding
â”‚   â”œâ”€â”€ config.py           â† Daemon config loader
â”‚   â””â”€â”€ ...
â”œâ”€â”€ messenger/
â”‚   â”œâ”€â”€ __init__.py         â† MessengerBot, MessageRouter, BotEvent
â”‚   â”œâ”€â”€ server.py           â† 24/7 bot server (Discord, Telegram)
â”‚   â”œâ”€â”€ discord.py          â† Discord connector
â”‚   â””â”€â”€ telegram.py         â† Telegram connector
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ mcp_filesystem_demo.py
â””â”€â”€ tests/
    â”œâ”€â”€ test_247_bot_setup.py   â† Setup flow tests (29 tests)
    â”œâ”€â”€ test_bot_factory.py     â† BotFactory / ReadyBot tests (40 tests)
    â””â”€â”€ ...
```

---

## ğŸ”— See Also

- [QUICKSTART.md](../QUICKSTART.md) â€” Get running in 5 minutes
- [INSTALL.md](../INSTALL.md) â€” Full installation guide
- [README.md](../README.md) â€” Project overview
- [MCP Protocol](https://modelcontextprotocol.io/)
