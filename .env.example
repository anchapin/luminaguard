# LuminaGuard Environment Configuration
# ======================================
# Copy this file to .env and fill in your values:
#
#   cp .env.example .env
#
# Then edit .env with your actual API keys and settings.
# NEVER commit .env to version control — it is listed in .gitignore.

# =============================================================================
# LLM Provider — set at least ONE of the sections below to enable AI responses
# =============================================================================
# The bot auto-detects which provider to use based on which variable is set.
# If none are set, the bot still runs but replies with a setup prompt.

# --- OpenAI (GPT-4, GPT-3.5, etc.) ------------------------------------------
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Optional: additional OpenAI keys used as automatic fallbacks when the
# primary key hits a quota or rate-limit error.  Add as many as you need.
# OPENAI_API_KEY_2=sk-…
# OPENAI_API_KEY_3=sk-…

# Optional: override the default model (default: gpt-4o-mini)
# OPENAI_MODEL=gpt-4o

# --- Anthropic (Claude) -------------------------------------------------------
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Optional: additional Anthropic keys used as automatic fallbacks.
# ANTHROPIC_API_KEY_2=sk-ant-…
# ANTHROPIC_API_KEY_3=sk-ant-…

# Optional: override the default model (default: claude-3-haiku-20240307)
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# --- Ollama (local, free, no API key required) --------------------------------
# Install Ollama: https://ollama.com/
# Then run: ollama pull llama3
# OLLAMA_HOST=http://localhost:11434

# Optional: override the default model (default: llama3)
# OLLAMA_MODEL=llama3

# --- Generic / fallback LLM key -----------------------------------------------
# Used when no provider-specific key is set above.
# LLM_API_KEY=

# =============================================================================
# Messenger Integrations (optional — enable only what you need)
# =============================================================================

# --- Discord ------------------------------------------------------------------
# Create a bot at: https://discord.com/developers/applications
# DISCORD_TOKEN=
# DISCORD_WEBHOOK_URL=https://your-domain.com/discord/webhook
# DISCORD_WEBHOOK_PORT=8080

# --- Telegram -----------------------------------------------------------------
# Create a bot via @BotFather on Telegram
# TELEGRAM_TOKEN=
# TELEGRAM_WEBHOOK_URL=https://your-domain.com/telegram/webhook
# TELEGRAM_WEBHOOK_SECRET=
# TELEGRAM_WEBHOOK_PORT=8081

# --- WhatsApp (Business API) --------------------------------------------------
# WHATSAPP_TOKEN=
# WHATSAPP_PHONE_ID=
# WHATSAPP_VERIFY_TOKEN=

# =============================================================================
# LuminaGuard Daemon Settings (optional — sensible defaults shown)
# =============================================================================

# Logging level: DEBUG | INFO | WARNING | ERROR | CRITICAL
# LUMINAGUARD_LOG_LEVEL=INFO

# Log output file (leave unset to log to stdout only)
# LUMINAGUARD_LOG_FILE=/var/log/luminaguard/agent.log

# Daemon host / port
# LUMINAGUARD_HOST=127.0.0.1
# LUMINAGUARD_PORT=8080

# PID and state files
# LUMINAGUARD_PID_FILE=/var/run/luminaguard.pid
# LUMINAGUARD_STATE_FILE=/var/run/luminaguard.state

# State persistence directory
# LUMINAGUARD_STATE_DIR=/var/lib/luminaguard/state

# Working directory for the daemon
# LUMINAGUARD_WORKING_DIRECTORY=/opt/luminaguard

# Approval timeout in seconds (default: 300 = 5 minutes)
# LUMINAGUARD_APPROVAL_TIMEOUT=300

# Agent execution mode: host | vm
# LUMINAGUARD_MODE=host

# VSock socket path (used when LUMINAGUARD_MODE=vm)
# LUMINAGUARD_VSOCK_PATH=/tmp/luminaguard.sock

# =============================================================================
# Development / Testing (optional)
# =============================================================================

# Set to 1 to run integration tests that make real network calls
# RUN_INTEGRATION_TESTS=0

# GitHub personal access token (used by GitHub MCP server integration tests)
# GH_TOKEN=

# Rust log verbosity
# RUST_LOG=info

# Enable Rust backtraces on panic
# RUST_BACKTRACE=1
